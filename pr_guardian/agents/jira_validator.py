"""
Agent 4 ‚Äî Jira Criteria Validator (LLM-powered).

R√©cup√®re la t√¢che Jira li√©e, extrait les acceptance criteria et
la definition of done, puis utilise Cohere LLM pour √©valuer
s√©mantiquement si le code impl√©mente chaque crit√®re.
"""

from __future__ import annotations

import json
import logging
from typing import Any

import cohere

from pr_guardian.agents.base_agent import BaseAgent
from pr_guardian.config import get_settings
from pr_guardian.integrations.jira_client import JiraClient
from pr_guardian.models import (
    AcceptanceCriterion,
    CheckStatus,
    CodeAnalysisResult,
    FigmaCheckResult,
    JiraValidationResult,
    PRContext,
    UMLCheckResult,
    Verdict,
)

logger = logging.getLogger("pr_guardian.agent.JiraValidator")

# ‚îÄ‚îÄ Prompt syst√®me pour la validation Jira ‚îÄ‚îÄ

JIRA_VALIDATOR_SYSTEM_PROMPT = """\
Tu es un QA Lead expert. Tu re√ßois :
1. Les acceptance criteria et definition of done d'une t√¢che Jira
2. Les r√©sultats d'analyse du code d'une Pull Request (classes, m√©thodes, endpoints, tests)
3. Le statut de v√©rification UML et Figma

Ton r√¥le : √©valuer s√©mantiquement si chaque crit√®re d'acceptation est satisfait par le code.

R√àGLES :
- Un crit√®re est PASS si le code contient clairement des √©l√©ments qui l'impl√©mentent
- Un crit√®re est FAIL si aucune preuve d'impl√©mentation n'est trouv√©e
- Un crit√®re est PARTIAL si l'impl√©mentation semble incompl√®te
- Ne pas halluciner : se baser UNIQUEMENT sur les preuves fournies
- √ätre strict mais juste dans l'√©valuation

R√©ponds UNIQUEMENT en JSON valide :
{
  "overall_score": <int 0-100>,
  "criteria_evaluations": [
    {
      "id": "AC-1 ou DoD-1",
      "description": "texte du crit√®re",
      "status": "PASS|FAIL|PARTIAL",
      "evidence": "preuves trouv√©es dans le code",
      "reasoning": "raisonnement d√©taill√©"
    }
  ],
  "recommended_verdict": "PASS|FAIL|BLOCKED",
  "summary": "r√©sum√© en 2-3 phrases"
}
"""


class JiraValidatorAgent(BaseAgent):
    """Agent 4 : valide les crit√®res Jira (statique + LLM)."""

    name = "JiraValidator"

    def __init__(self, jira_client: JiraClient | None = None):
        super().__init__()
        self._jira = jira_client
        self._settings = get_settings()

    def _get_jira(self) -> JiraClient:
        if self._jira is None:
            self._jira = JiraClient()
        return self._jira

    async def run(
        self,
        context: PRContext,
        code_analysis: CodeAnalysisResult | None = None,
        uml_check: UMLCheckResult | None = None,
        figma_check: FigmaCheckResult | None = None,
        **kwargs: Any,
    ) -> JiraValidationResult:
        self._log_start(context)
        result = JiraValidationResult()

        # ‚îÄ‚îÄ V√©rifier la cl√© Jira ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        jira_key = context.jira_key
        if not jira_key:
            self._log_blocked("Aucune cl√© Jira trouv√©e dans le contexte.")
            result.status = CheckStatus.BLOCKED
            result.summary = "Aucune cl√© Jira associ√©e √† cette PR."
            result.recommended_verdict = Verdict.BLOCKED
            return result

        result.jira_key = jira_key

        # ‚îÄ‚îÄ R√©cup√©rer l'issue Jira ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            jira = self._get_jira()
            fields = jira.get_issue_fields(jira_key)
        except Exception as exc:
            self._log_blocked(f"Impossible de r√©cup√©rer l'issue Jira {jira_key}: {exc}")
            result.status = CheckStatus.BLOCKED
            result.summary = f"Erreur d'acc√®s √† Jira pour {jira_key}: {exc}"
            result.recommended_verdict = Verdict.BLOCKED
            return result

        result.jira_summary = fields.get("summary", "")
        result.jira_description = fields.get("description", "")
        result.jira_status = fields.get("status", "")

        # ‚îÄ‚îÄ Extraire les Acceptance Criteria ‚îÄ‚îÄ
        ac_texts = fields.get("acceptance_criteria", [])
        dod_texts = fields.get("definition_of_done", [])

        if not ac_texts and not dod_texts:
            result.status = CheckStatus.PARTIAL
            result.summary = (
                f"Issue {jira_key} trouv√©e, mais aucun acceptance criteria "
                "ni definition of done d√©tect√©."
            )
            result.recommended_verdict = Verdict.BLOCKED
            return result

        # ‚îÄ‚îÄ Tenter l'√©valuation LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        llm_result = None
        if self._settings.llm_configured and code_analysis:
            llm_result = self._llm_evaluate_criteria(
                ac_texts, dod_texts, code_analysis, uml_check, figma_check, context
            )

        if llm_result:
            # Appliquer les r√©sultats LLM
            self._apply_llm_result(result, llm_result, ac_texts, dod_texts)
        else:
            # Fallback statique
            self._evaluate_static(result, ac_texts, dod_texts, code_analysis, uml_check, figma_check)

        self._log_done(context)
        return result

    def _llm_evaluate_criteria(
        self,
        ac_texts: list[str],
        dod_texts: list[str],
        code: CodeAnalysisResult,
        uml: UMLCheckResult | None,
        figma: FigmaCheckResult | None,
        context: PRContext,
    ) -> dict | None:
        """Appelle Cohere pour √©valuer s√©mantiquement chaque crit√®re."""
        try:
            # Formater les crit√®res
            criteria_text = ""
            for i, ac in enumerate(ac_texts, 1):
                criteria_text += f"AC-{i}: {ac}\n"
            for i, dod in enumerate(dod_texts, 1):
                criteria_text += f"DoD-{i}: {dod}\n"

            # Formater les preuves
            code_info = (
                f"Classes : {', '.join(code.classes_touched) or 'aucune'}\n"
                f"M√©thodes : {', '.join(code.methods_touched) or 'aucune'}\n"
                f"Endpoints : {', '.join(code.endpoints) or 'aucun'}\n"
                f"Fichiers modifi√©s : {', '.join(f.filename for f in code.files_modified[:20])}\n"
                f"Features : {', '.join(code.features_detected) or 'aucune'}\n"
                f"Tests ajout√©s : {', '.join(code.tests_added) or 'aucun'}\n"
                f"Tests modifi√©s : {', '.join(code.tests_modified) or 'aucun'}\n"
                f"Couverture : {code.test_coverage_info}"
            )

            extra_info = ""
            if uml:
                extra_info += f"\nUML : statut={uml.status.value}, {len(uml.mismatches)} √©cart(s)"
            if figma:
                extra_info += f"\nFigma : statut={figma.status.value}, {len(figma.mappings)} mapping(s)"

            user_message = (
                f"PR: {context.repo} #{context.pr_number} ‚Äî {context.pr_title}\n\n"
                f"## CRIT√àRES √Ä √âVALUER\n{criteria_text}\n"
                f"## CODE DE LA PR\n{code_info}\n"
                f"## CONTEXTE SUPPL√âMENTAIRE{extra_info}"
            )

            client = cohere.ClientV2(api_key=self._settings.cohere_api_key)
            response = client.chat(
                model=self._settings.cohere_model,
                messages=[
                    {"role": "system", "content": JIRA_VALIDATOR_SYSTEM_PROMPT},
                    {"role": "user", "content": user_message},
                ],
                max_tokens=2048,
                temperature=0.1,
                response_format={"type": "json_object"},
            )

            raw = response.message.content[0].text if response.message.content else "{}"
            analysis = json.loads(raw)
            logger.info("[JiraValidator] LLM evaluation OK ‚Äî score: %s/100",
                        analysis.get("overall_score", "?"))
            return analysis

        except Exception as exc:
            logger.warning("[JiraValidator] LLM evaluation failed: %s", exc)
            return None

    def _apply_llm_result(
        self,
        result: JiraValidationResult,
        llm: dict,
        ac_texts: list[str],
        dod_texts: list[str],
    ) -> None:
        """Convertit la r√©ponse LLM en AcceptanceCriterion + verdict."""
        status_map = {
            "PASS": CheckStatus.PASS,
            "OK": CheckStatus.PASS,
            "FAIL": CheckStatus.FAIL,
            "PARTIAL": CheckStatus.PARTIAL,
        }

        evaluations = {e.get("id", ""): e for e in llm.get("criteria_evaluations", [])}

        # AC
        for i, ac_text in enumerate(ac_texts, 1):
            cid = f"AC-{i}"
            llm_eval = evaluations.get(cid, {})
            criterion = AcceptanceCriterion(
                id=cid,
                description=ac_text,
                status=status_map.get(llm_eval.get("status", "FAIL"), CheckStatus.FAIL),
                evidence=llm_eval.get("evidence", llm_eval.get("reasoning", "")),
            )
            result.acceptance_criteria.append(criterion)

        # DoD
        for i, dod_text in enumerate(dod_texts, 1):
            cid = f"DoD-{i}"
            llm_eval = evaluations.get(cid, {})
            criterion = AcceptanceCriterion(
                id=cid,
                description=dod_text,
                status=status_map.get(llm_eval.get("status", "FAIL"), CheckStatus.FAIL),
                evidence=llm_eval.get("evidence", llm_eval.get("reasoning", "")),
            )
            result.definition_of_done.append(criterion)

        # Verdict
        all_criteria = result.acceptance_criteria + result.definition_of_done
        fail_count = sum(1 for c in all_criteria if c.status == CheckStatus.FAIL)
        pass_count = sum(1 for c in all_criteria if c.status in (CheckStatus.PASS, CheckStatus.OK))
        partial_count = sum(1 for c in all_criteria if c.status == CheckStatus.PARTIAL)

        verdict_map = {"PASS": Verdict.PASS, "FAIL": Verdict.FAIL, "BLOCKED": Verdict.BLOCKED}
        llm_verdict = verdict_map.get(llm.get("recommended_verdict", ""), None)

        if llm_verdict:
            result.recommended_verdict = llm_verdict
        elif fail_count > 0:
            result.recommended_verdict = Verdict.FAIL
        elif partial_count > 0:
            result.recommended_verdict = Verdict.FAIL
        else:
            result.recommended_verdict = Verdict.PASS

        if fail_count > 0:
            result.status = CheckStatus.FAIL
        elif partial_count > 0:
            result.status = CheckStatus.PARTIAL
        else:
            result.status = CheckStatus.OK

        result.summary = (
            f"ü§ñ Issue {result.jira_key} ‚Äî {len(all_criteria)} crit√®re(s) √©valu√©(s) par IA : "
            f"{pass_count} PASS, {fail_count} FAIL, {partial_count} PARTIAL."
        )
        if llm.get("summary"):
            result.summary += f"\n{llm['summary']}"

    def _evaluate_static(
        self,
        result: JiraValidationResult,
        ac_texts: list[str],
        dod_texts: list[str],
        code: CodeAnalysisResult | None,
        uml: UMLCheckResult | None,
        figma: FigmaCheckResult | None,
    ) -> None:
        """Fallback statique : √©valuation par mots-cl√©s."""
        for i, ac_text in enumerate(ac_texts, 1):
            criterion = AcceptanceCriterion(
                id=f"AC-{i}",
                description=ac_text,
            )
            criterion.status, criterion.evidence = self._evaluate_criterion(
                ac_text, code, uml, figma
            )
            result.acceptance_criteria.append(criterion)

        for i, dod_text in enumerate(dod_texts, 1):
            criterion = AcceptanceCriterion(
                id=f"DoD-{i}",
                description=dod_text,
            )
            criterion.status, criterion.evidence = self._evaluate_criterion(
                dod_text, code, uml, figma
            )
            result.definition_of_done.append(criterion)

        # Verdict
        all_criteria = result.acceptance_criteria + result.definition_of_done
        fail_count = sum(1 for c in all_criteria if c.status == CheckStatus.FAIL)
        pass_count = sum(1 for c in all_criteria if c.status in (CheckStatus.PASS, CheckStatus.OK))
        partial_count = sum(1 for c in all_criteria if c.status == CheckStatus.PARTIAL)

        if fail_count > 0:
            result.status = CheckStatus.FAIL
            result.recommended_verdict = Verdict.FAIL
        elif partial_count > 0:
            result.status = CheckStatus.PARTIAL
            result.recommended_verdict = Verdict.FAIL
        else:
            result.status = CheckStatus.OK
            result.recommended_verdict = Verdict.PASS

        result.summary = (
            f"Issue {result.jira_key} ‚Äî {len(all_criteria)} crit√®re(s) √©valu√©(s) : "
            f"{pass_count} PASS, {fail_count} FAIL, {partial_count} PARTIAL."
        )

    @staticmethod
    def _evaluate_criterion(
        criterion_text: str,
        code: CodeAnalysisResult | None,
        uml: UMLCheckResult | None,
        figma: FigmaCheckResult | None,
    ) -> tuple[CheckStatus, str]:
        """
        Fallback statique : √©value un crit√®re via recherche de mots-cl√©s.
        """
        if not code:
            return CheckStatus.BLOCKED, "Pas d'analyse de code disponible."

        criterion_lower = criterion_text.lower()
        evidence_parts: list[str] = []
        matched = False

        for ep in code.endpoints:
            if _keyword_overlap(criterion_lower, ep.lower()):
                evidence_parts.append(f"Endpoint trouv√© : {ep}")
                matched = True

        for cls in code.classes_touched:
            if _keyword_overlap(criterion_lower, cls.lower()):
                evidence_parts.append(f"Classe touch√©e : {cls}")
                matched = True

        for feat in code.features_detected:
            if _keyword_overlap(criterion_lower, feat.lower()):
                evidence_parts.append(f"Fonctionnalit√© : {feat}")
                matched = True

        if any(kw in criterion_lower for kw in ("test", "couverture", "coverage")):
            if code.tests_added or code.tests_modified:
                evidence_parts.append(
                    f"Tests : {len(code.tests_added)} ajout√©(s), "
                    f"{len(code.tests_modified)} modifi√©(s)"
                )
                matched = True

        if uml and any(kw in criterion_lower for kw in ("diagramme", "uml", "architecture")):
            if uml.status == CheckStatus.OK:
                evidence_parts.append("UML coh√©rent.")
                matched = True
            elif uml.mismatches:
                evidence_parts.append(f"UML : {len(uml.mismatches)} √©cart(s)")

        if figma and any(kw in criterion_lower for kw in ("ui", "interface", "design", "figma",
                                                            "√©cran", "composant", "maquette")):
            if figma.status == CheckStatus.OK:
                evidence_parts.append("Figma conforme.")
                matched = True
            elif figma.mappings:
                fail_maps = [m for m in figma.mappings if m.implementation_status == CheckStatus.FAIL]
                if fail_maps:
                    evidence_parts.append(f"Figma : {len(fail_maps)} √©cart(s)")

        if matched:
            return CheckStatus.PASS, " | ".join(evidence_parts)
        elif evidence_parts:
            return CheckStatus.PARTIAL, " | ".join(evidence_parts)
        else:
            return CheckStatus.FAIL, "Aucune preuve trouv√©e dans le code pour ce crit√®re."


def _keyword_overlap(text_a: str, text_b: str) -> bool:
    """V√©rifie s'il y a un chevauchement significatif de mots-cl√©s."""
    words_a = {w for w in text_a.split() if len(w) >= 4}
    words_b = {w for w in text_b.split() if len(w) >= 4}
    if not words_a or not words_b:
        return False
    overlap = words_a & words_b
    return len(overlap) >= 1
